{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection II - selecting for model accuracy\n",
    "> In this second chapter on feature selection, you'll learn how to let models help you find the most important features in a dataset for predicting a particular target feature. In the final lesson of this chapter, you'll combine the advice of multiple, different, models to decide on which features are worth keeping. This is the Summary of lecture \"Dimensionality Reduction in Python\", via datacamp.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Chanseok Kang\n",
    "- categories: [Python, Datacamp, Machine Learning]\n",
    "- image: images/mse.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting features for model performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a diabetes classifier\n",
    "You'll be using the Pima Indians diabetes dataset to predict whether a person has diabetes using logistic regression. There are 8 features and one target in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df = pd.read_csv('./dataset/PimaIndians.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pprint import pprint\n",
    "\n",
    "X, y = diabetes_df.iloc[:, :-1], diabetes_df.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.9% accuracy on test set.\n",
      "{'age': 0.21,\n",
      " 'bmi': 0.17,\n",
      " 'diastolic': 0.12,\n",
      " 'family': 0.22,\n",
      " 'glucose': 1.13,\n",
      " 'insulin': 0.11,\n",
      " 'pregnant': 0.29,\n",
      " 'triceps': 0.28}\n"
     ]
    }
   ],
   "source": [
    "# Fit the scaler on the training features and transform these in one go\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# Fit the logistic regression model on the scaled training data\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "# Scaler the test features\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Predict diabetes presence on the scaled test set\n",
    "y_pred = lr.predict(X_test_std)\n",
    "\n",
    "# Print accuracy metrics and feature coefficients\n",
    "print(\"{0:.1%} accuracy on test set.\".format(accuracy_score(y_test, y_pred)))\n",
    "pprint(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get almost 80% accuracy on the test set. Take a look at the differences in model coefficients for the different features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Recursive Feature Elimination\n",
    "Now that we've created a diabetes classifier, let's see if we can reduce the number of features without hurting the model accuracy too much.\n",
    "\n",
    "On the second line of code the features are selected from the original dataframe. Adjust this selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80.6% accuracy on test set.\n",
      "{'age': 0.35,\n",
      " 'bmi': 0.39,\n",
      " 'family': 0.34,\n",
      " 'glucose': 1.24,\n",
      " 'insulin': 0.2,\n",
      " 'pregnant': 0.05,\n",
      " 'triceps': 0.24}\n"
     ]
    }
   ],
   "source": [
    "# Remove the feature with the lowest model coefficient\n",
    "X = diabetes_df[['pregnant', 'glucose', 'triceps',\n",
    "                 'insulin', 'bmi', 'family', 'age']]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculate the accuracy on the test set and prints coefficients\n",
    "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "print(\"{0: .1%} accuracy on test set.\".format(acc))\n",
    "pprint(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.6% accuracy on test set.\n",
      "{'age': 0.37, 'bmi': 0.34, 'family': 0.34, 'glucose': 1.13, 'triceps': 0.25}\n"
     ]
    }
   ],
   "source": [
    "# Remove the 2 features with the lowest model coefficients\n",
    "X = diabetes_df[['glucose', 'triceps', 'bmi', 'family', 'age']]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) \n",
    "pprint(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.5% accuracy on test set.\n",
      "{'glucose': 1.28}\n"
     ]
    }
   ],
   "source": [
    "# Only keep the feature with the highest coefficient\n",
    "X = diabetes_df[['glucose']]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model to the data\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) \n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Recursive Feature Elimination\n",
    "Now let's automate this recursive process. Wrap a Recursive Feature Eliminator (RFE) around our logistic regression estimator and pass it the desired number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = diabetes_df.iloc[:, :-1], diabetes_df.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Fit the scaler on the training features and transform these in one go\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# Fit the logistic regression model on the scaled training data\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "# Scaler the test features\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "{'pregnant': 3, 'glucose': 1, 'diastolic': 4, 'triceps': 6, 'insulin': 5, 'bmi': 2, 'family': 1, 'age': 1}\n",
      "Index(['glucose', 'family', 'age'], dtype='object')\n",
      "75.4% accuracy on test set.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Create the RFE a LogisticRegression estimator and 3 features to select\n",
    "rfe = RFE(estimator=LogisticRegression(), n_features_to_select=3, verbose=1)\n",
    "\n",
    "# Fits the eliminator to the data\n",
    "rfe.fit(X_train_std, y_train)\n",
    "\n",
    "# Print the features and their ranking (high = dropped early on)\n",
    "print(dict(zip(X.columns, rfe.ranking_)))\n",
    "\n",
    "# Print the features that are not elimiated\n",
    "print(X.columns[rfe.support_])\n",
    "\n",
    "# CAlculates the test set accuracy\n",
    "acc = accuracy_score(y_test, rfe.predict(X_test_std))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-based feature selection\n",
    "- Random forest classifier\n",
    "![rf classifier](image/rfc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a random forest model\n",
    "You'll again work on the Pima Indians dataset to predict whether an individual has diabetes. This time using a random forest classifier. You'll fit the model on the training data after performing the train-test split and consult the feature importance values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': 0.13,\n",
      " 'bmi': 0.12,\n",
      " 'diastolic': 0.09,\n",
      " 'family': 0.12,\n",
      " 'glucose': 0.25,\n",
      " 'insulin': 0.14,\n",
      " 'pregnant': 0.07,\n",
      " 'triceps': 0.09}\n",
      "79.6% accuracy on test set.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Perform a 75% training and 25% test data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Fit the random forest model to the training data\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the accuracy\n",
    "acc = accuracy_score(y_test, rf.predict(X_test))\n",
    "\n",
    "# Print the importances per feature\n",
    "pprint(dict(zip(X.columns, rf.feature_importances_.round(2))))\n",
    "\n",
    "# Print accuracy\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest for feature selection\n",
    "Now lets use the fitted random model to select the most important features from our input dataset `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True False False False False False False]\n",
      "Index(['glucose'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Create a mask for features importances above the threshold\n",
    "mask = rf.feature_importances_ > 0.15\n",
    "\n",
    "# Prints out the mask\n",
    "print(mask)\n",
    "\n",
    "# Apply the mask to the feature dataset X\n",
    "reduced_X = X.loc[:, mask]\n",
    "\n",
    "# Prints out the selected column names\n",
    "print(reduced_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination with random forests\n",
    "You'll wrap a Recursive Feature Eliminator around a random forest model to remove features step by step. This method is more conservative compared to selecting features after applying a single importance threshold. Since dropping one feature can influence the relative importances of the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Index(['glucose', 'bmi'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Wrap the feature eliminator around the random forest model\n",
    "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, verbose=1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Create a mask using an attribute of rfe\n",
    "mask = rfe.support_\n",
    "\n",
    "# Apply the mask to the feature dataset X and print the result\n",
    "reduced_X = X.loc[:, mask]\n",
    "print(reduced_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 4 features.\n",
      "Index(['glucose', 'age'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Wrap the feature eliminator around the random forest model\n",
    "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, step=2, verbose=1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Create a mask using an attribute of rfe\n",
    "mask = rfe.support_\n",
    "\n",
    "# Apply the mask to the feature dataset X and print the result\n",
    "reduced_X = X.loc[:, mask]\n",
    "print(reduced_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the quick and dirty single threshold method from the previous exercise one of the selected features is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized linear regression\n",
    "- Loss function: Mean Squared Error\n",
    "![mse](image/mse.png)\n",
    "- Adding regularization\n",
    "$$ \\text{MSE} + \\overbrace{\\alpha(\\vert \\beta_1 \\vert + \\vert \\beta_2 \\vert + \\vert \\beta_3 \\vert)}^{\\text{Regularization term}} $$\n",
    "    - MSE tries to make model accurate\n",
    "    - Regularization term tries to make model simple\n",
    "    - $\\alpha$, when it's too low, the model might overfit. when it's too high, the model might become too simple and inaccurate. One linear model that includes this type of regularization is called **Lasso**, for **L**east **A**bsolute **S**hrinkage and **S**election."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a LASSO regressor\n",
    "You'll be working on the numeric ANSUR body measurements dataset to predict a persons Body Mass Index (BMI) using the `Lasso()` regressor. BMI is a metric derived from body height and weight but those two features have been removed from the dataset to give the model a challenge.\n",
    "\n",
    "You'll standardize the data first using the `StandardScaler()` that has been instantiated for you as scaler to make sure all coefficients face a comparable regularizing force trying to bring them down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ansur_male = pd.read_csv('./dataset/ANSUR_II_MALE.csv')\n",
    "\n",
    "ansur_df = ansur_male\n",
    "# unused columns in the dataset\n",
    "unused = ['Gender', 'Branch', 'Component', 'BMI_class', 'Height_class', 'weight_kg', 'stature_m']\n",
    "\n",
    "# Drop the non-numeric columns from df\n",
    "ansur_df.drop(unused, axis=1, inplace=True)\n",
    "\n",
    "X = ansur_df.drop('BMI', axis=1)\n",
    "y = ansur_df['BMI']\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "      normalize=False, positive=False, precompute=False, random_state=None,\n",
       "      selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Set the test size to 30% to get a 70-30% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Fit the scaler on the training features and transform these in one go\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# Create the Lasso model\n",
    "la = Lasso()\n",
    "\n",
    "# Fit it to the standardized training data\n",
    "la.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso model results\n",
    "Now that you've trained the Lasso model, you'll score its predictive capacity ($R^2$) on the test set and count how many features are ignored because their coefficient is reduced to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model can predict 84.7% of the variance in the test set.\n",
      "The model has ignored 82 out of 91 features.\n"
     ]
    }
   ],
   "source": [
    "# Transform the test set with the pre-fitted scaler\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Calculate the coefficient of determination (R squared) on X_test_std\n",
    "r_squared = la.score(X_test_std, y_test)\n",
    "print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
    "\n",
    "# Create a list that has True values when coefficients equal 0\n",
    "zero_coef = la.coef_ == 0\n",
    "\n",
    "# Calculate how many features have a zero coefficient\n",
    "n_ignored = sum(zero_coef)\n",
    "print(\"The model has ignored {} out of {} features.\".format(n_ignored, len(la.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can predict almost 85% of the variance in the BMI value using just 9 out of 91 of the features. The $R^2$ could be higher though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting the regularization strength\n",
    "Your current Lasso model has an $R^2$ score of 84.7%. When a model applies overly powerful regularization it can suffer from high bias, hurting its predictive power.\n",
    "\n",
    "Let's improve the balance between predictive power and model simplicity by tweaking the `alpha` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model can predict 84.7% of the variance in the test set.\n",
      "82 out of 91 features were ignored.\n",
      "The model can predict 93.8% of the variance in the test set.\n",
      "79 out of 91 features were ignored.\n",
      "The model can predict 98.3% of the variance in the test set.\n",
      "64 out of 91 features were ignored.\n",
      "Max R-squared: 0.9828190248586458, alpha: 0.1\n"
     ]
    }
   ],
   "source": [
    "alpha_list = [1, 0.5, 0.1, 0.01]\n",
    "max_r = 0\n",
    "max_alpha = 0\n",
    "for alpha in alpha_list:\n",
    "    # Find the highest alpha value with R-squared above 98%\n",
    "    la = Lasso(alpha=alpha, random_state=0)\n",
    "\n",
    "    # Fits the model and calculates performance stats\n",
    "    la.fit(X_train_std, y_train)\n",
    "    r_squared = la.score(X_test_std, y_test)\n",
    "    n_ignored_features = sum(la.coef_ == 0)\n",
    "\n",
    "    # Print peformance stats \n",
    "    print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
    "    print(\"{} out of {} features were ignored.\".format(n_ignored_features, len(la.coef_)))\n",
    "    if r_squared > 0.98:\n",
    "        max_r = r_squared\n",
    "        max_alpha = alpha\n",
    "        break\n",
    "print(\"Max R-squared: {}, alpha: {}\".format(max_r, max_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining feature selectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a LassoCV regressor\n",
    "You'll be predicting biceps circumference on a subsample of the male ANSUR dataset using the `LassoCV()` regressor that automatically tunes the regularization strength (alpha value) using Cross-Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ansur_df[['acromialheight', 'axillaheight', 'bideltoidbreadth', 'buttockcircumference', 'buttockkneelength', 'buttockpopliteallength', 'cervicaleheight', 'chestcircumference', 'chestheight',\n",
    "       'earprotrusion', 'footbreadthhorizontal', 'forearmcircumferenceflexed', 'handlength', 'headbreadth', 'heelbreadth', 'hipbreadth', 'iliocristaleheight', 'interscyeii',\n",
    "       'lateralfemoralepicondyleheight', 'lateralmalleolusheight', 'neckcircumferencebase', 'radialestylionlength', 'shouldercircumference', 'shoulderelbowlength', 'sleeveoutseam',\n",
    "       'thighcircumference', 'thighclearance', 'verticaltrunkcircumferenceusa', 'waistcircumference', 'waistdepth', 'wristheight', 'BMI']]\n",
    "y = ansur_df['bicepscircumferenceflexed']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha = 0.035\n",
      "The model explains 85.6% of the test set variance\n",
      "24 features out of 32 selected\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Create and fit the LassoCV model on the training set\n",
    "lcv = LassoCV()\n",
    "lcv.fit(X_train, y_train)\n",
    "print('Optimal alpha = {0:.3f}'.format(lcv.alpha_))\n",
    "\n",
    "# Calculate R squared on the test set\n",
    "r_squared = lcv.score(X_test, y_test)\n",
    "print('The model explains {0:.1%} of the test set variance'.format(r_squared))\n",
    "\n",
    "# Create a mask for coefficients not equal to zero\n",
    "lcv_mask = lcv.coef_ != 0\n",
    "print('{} features out of {} selected'.format(sum(lcv_mask), len(lcv_mask)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble models for extra votes\n",
    "The LassoCV() model selected 24 out of 32 features. Not bad, but not a spectacular dimensionality reduction either. Let's use two more models to select the 10 features they consider most important using the Recursive Feature Eliminator (RFE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 11 features.\n",
      "The model can explain 83.3% of the variance in the test set\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\n",
    "rfe_gb = RFE(estimator=GradientBoostingRegressor(),\n",
    "            n_features_to_select=10, step=3, verbose=1)\n",
    "rfe_gb.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the R squared on the test set\n",
    "r_squared = rfe_gb.score(X_test, y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
    "\n",
    "# Assign the support array to gb_mask\n",
    "gb_mask = rfe_gb.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 11 features.\n",
      "The model can explain 82.3% of the variance in the test set\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step\n",
    "rfe_rf = RFE(estimator=RandomForestRegressor(),\n",
    "            n_features_to_select=10, step=3, verbose=1)\n",
    "rfe_rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the R squared on the test set\n",
    "r_squared = rfe_rf.score(X_test, y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
    "\n",
    "# Assign the support array to rf_mask\n",
    "rf_mask = rfe_rf.support_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inluding the Lasso linear model from the previous exercise, we now have the votes from 3 models on which features are important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining 3 feature selectors\n",
    "We'll combine the votes of the 3 models you built in the previous exercises, to decide which features are important into a meta mask. We'll then use this mask to reduce dimensionality and see how a simple linear regressor performs on the reduced dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 3 3 0 1 1 3 1 1 1 3 1 1 1 0 0 1 0 1 1 1 3 3 0 3 2 2 1 2 0 3]\n",
      "[False False  True  True False False False  True False False False  True\n",
      " False False False False False False False False False False  True  True\n",
      " False  True False False False False False  True]\n",
      "Index(['bideltoidbreadth', 'buttockcircumference', 'chestcircumference',\n",
      "       'forearmcircumferenceflexed', 'shouldercircumference',\n",
      "       'shoulderelbowlength', 'thighcircumference', 'BMI'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Sum the votes of the three models\n",
    "votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
    "print(votes)\n",
    "\n",
    "# Create a mask for features selected by all 3 models\n",
    "meta_mask = votes == 3\n",
    "print(meta_mask)\n",
    "\n",
    "# Apply the dimensionality reduction on X\n",
    "X_reduced = X.loc[:, meta_mask]\n",
    "print(X_reduced.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model can explain 84.0% of the variance in the test set using 8 features.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Plug the reduced data into a linear regression pipeline\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\n",
    "lm.fit(scaler.fit_transform(X_train), y_train)\n",
    "r_squared = lm.score(scaler.transform(X_test), y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set using {1:} features.'.format(r_squared, len(lm.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the votes from 3 models you were able to select just 7 features that allowed a simple linear model to get a high accuracy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
